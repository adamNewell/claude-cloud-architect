[
  {
    "id": "MLPER01-BP01",
    "title_full": "MLPER01-BP01 Select appropriate instance types for training workloads",
    "title": "Select appropriate instance types for training workloads",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-01.html",
    "description": "Choose optimal instance types for training based on workload characteristics including model architecture, dataset size, and training framework. Use GPU instances for deep learning workloads and consider specialized instances like AWS Trainium for cost-effective training. Benchmark different instance families to identify the best price-performance ratio.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Compute selection"],
    "relatedIds": ["PERF03-BP01"],
    "risk": "HIGH",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER01-BP02",
    "title_full": "MLPER01-BP02 Select appropriate instance types for inference workloads",
    "title": "Select appropriate instance types for inference workloads",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-01.html",
    "description": "Choose inference instance types based on latency requirements, throughput needs, and cost constraints. Consider AWS Inferentia instances for cost-effective inference at scale. Use multi-model endpoints to maximize instance utilization for multiple models.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Compute selection"],
    "relatedIds": ["PERF03-BP02"],
    "risk": "HIGH",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER01-BP03",
    "title_full": "MLPER01-BP03 Use distributed training for large-scale models",
    "title": "Use distributed training for large-scale models",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-01.html",
    "description": "Implement distributed training strategies for large models or datasets using SageMaker distributed training libraries. Use data parallelism for large datasets and model parallelism for models that exceed single GPU memory. Optimize inter-node communication with appropriate instance placement.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Compute selection"],
    "relatedIds": ["PERF03-BP03"],
    "risk": "MEDIUM",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER02-BP01",
    "title_full": "MLPER02-BP01 Optimize data loading and preprocessing pipelines",
    "title": "Optimize data loading and preprocessing pipelines",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-02.html",
    "description": "Implement efficient data loading using SageMaker Pipe mode or FastFile mode to stream data directly from S3. Use parallel data loading and preprocessing to maximize GPU utilization. Convert data to optimized formats like RecordIO or TFRecord for faster training.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Data optimization"],
    "relatedIds": ["PERF04-BP01"],
    "risk": "MEDIUM",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER02-BP02",
    "title_full": "MLPER02-BP02 Use feature stores for efficient feature serving",
    "title": "Use feature stores for efficient feature serving",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-02.html",
    "description": "Implement Amazon SageMaker Feature Store for low-latency feature retrieval during inference. Use online stores for real-time features and offline stores for batch processing. Optimize feature retrieval patterns to minimize latency impact on inference.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Data optimization"],
    "relatedIds": ["PERF04-BP02"],
    "risk": "MEDIUM",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER03-BP01",
    "title_full": "MLPER03-BP01 Optimize model architecture for inference performance",
    "title": "Optimize model architecture for inference performance",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-03.html",
    "description": "Apply model optimization techniques including quantization, pruning, and knowledge distillation to reduce model size and improve inference speed. Use SageMaker Neo to compile models for target hardware. Balance model accuracy against latency requirements.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Model optimization"],
    "relatedIds": ["PERF02-BP01"],
    "risk": "MEDIUM",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER03-BP02",
    "title_full": "MLPER03-BP02 Implement model caching and batching for inference",
    "title": "Implement model caching and batching for inference",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-03.html",
    "description": "Use model caching to reduce loading time for frequently accessed models. Implement request batching to improve throughput by processing multiple inference requests together. Configure optimal batch sizes based on latency SLAs and throughput requirements.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Model optimization"],
    "relatedIds": ["PERF02-BP02"],
    "risk": "MEDIUM",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER04-BP01",
    "title_full": "MLPER04-BP01 Monitor and optimize training job performance",
    "title": "Monitor and optimize training job performance",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-04.html",
    "description": "Use SageMaker Debugger to profile training jobs and identify performance bottlenecks. Monitor GPU utilization, memory usage, and I/O patterns. Optimize hyperparameters and batch sizes to maximize hardware utilization and reduce training time.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Performance monitoring"],
    "relatedIds": ["PERF02-BP03"],
    "risk": "MEDIUM",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER04-BP02",
    "title_full": "MLPER04-BP02 Monitor and optimize inference endpoint performance",
    "title": "Monitor and optimize inference endpoint performance",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-04.html",
    "description": "Monitor inference endpoint metrics including model latency, invocation errors, and GPU utilization. Use CloudWatch metrics and SageMaker Model Monitor for performance tracking. Set up alarms for latency SLA violations and implement continuous performance optimization.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Performance monitoring"],
    "relatedIds": ["PERF02-BP04"],
    "risk": "HIGH",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER05-BP01",
    "title_full": "MLPER05-BP01 Use automatic model tuning for hyperparameter optimization",
    "title": "Use automatic model tuning for hyperparameter optimization",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-05.html",
    "description": "Implement SageMaker Automatic Model Tuning to find optimal hyperparameters efficiently. Use Bayesian optimization or random search strategies based on search space characteristics. Define appropriate objective metrics and early stopping conditions to reduce tuning costs.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Optimization techniques"],
    "relatedIds": ["PERF01-BP01"],
    "risk": "MEDIUM",
    "lens": "MACHINE_LEARNING"
  },
  {
    "id": "MLPER05-BP02",
    "title_full": "MLPER05-BP02 Implement serverless inference for variable workloads",
    "title": "Implement serverless inference for variable workloads",
    "href": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-05.html",
    "description": "Use SageMaker Serverless Inference for workloads with intermittent or unpredictable traffic patterns. Configure memory size and maximum concurrency based on model requirements. Accept cold start latency tradeoffs in exchange for cost savings on low-traffic endpoints.",
    "outcome": "",
    "pillar": "PERFORMANCE_EFFICIENCY",
    "area": ["Optimization techniques"],
    "relatedIds": ["PERF01-BP02"],
    "risk": "LOW",
    "lens": "MACHINE_LEARNING"
  }
]
